p8105_hw5_xc2784
================
2025-11-14

### problem 1

``` r
duplicate_birthday <- function(n, days = 365) {
  birthdays <- sample.int(days, n, replace = TRUE)
  any(duplicated(birthdays))
}
```

``` r
sim_prob <- function(n, reps = 10000L) {
  mean(replicate(reps, duplicate_birthday(n)))
}

group_sizes <- 2:50
prob_shared <- sapply(group_sizes, sim_prob)

plot(group_sizes, prob_shared,
     xlab = "Group size (n)",
     ylab = "Probability of at least two people sharing a birthday",
     main = "Birthday Simulation (10,000 Replications)")
```

![](p8105_hw5_xc2784_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

The plot shows the probability that at least two people in a group share
a birthday as the group size increases from 2 to 50. The curve starts
near zero for very small groups and rises slowly at first, but then
increases rapidly between group sizes of about 15 to 30, and approaches
1 when the group size grows to about 50.

### problem 2

``` r
set.seed(10)
n <- 30
sigma <- 5
mu_values <- 0:6
reps <- 5000L
alpha <- 0.05
```

``` r
simulate1 <- function(mu, n, sigma) {
    x <- rnorm(n, mean = mu, sd = sigma)
    test <- t.test(x, mu = 0)
    tibble(
    mu      = mu,
    mu_hat  = mean(x),
    p_value = tidy(test)$p.value
  )
}

simulation_results <- mu_values %>%  
  map_df(~ bind_rows(replicate(reps, simulate1(.x, n, sigma), simplify = FALSE))
)

dplyr::glimpse(simulation_results)
```

    ## Rows: 35,000
    ## Columns: 3
    ## $ mu      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
    ## $ mu_hat  <dbl> -1.72338223, -0.56433529, -0.06373376, 0.82176074, -0.54577899‚Ä¶
    ## $ p_value <dbl> 0.03769650, 0.51941790, 0.94476785, 0.33832357, 0.55589101, 0.‚Ä¶

``` r
summary_results <- simulation_results %>%
  mutate(reject = p_value < alpha) %>% 
  group_by(mu) %>%
  summarise(
    power = mean(reject),
    avg_mu_hat = mean(mu_hat),
    avg_mu_hat_reject = mean(mu_hat[reject])
  )

dplyr::glimpse(summary_results)
```

    ## Rows: 7
    ## Columns: 4
    ## $ mu                <int> 0, 1, 2, 3, 4, 5, 6
    ## $ power             <dbl> 0.0484, 0.1884, 0.5542, 0.8876, 0.9882, 0.9994, 1.00‚Ä¶
    ## $ avg_mu_hat        <dbl> -0.02322427, 1.01178046, 1.98914725, 2.99978138, 3.9‚Ä¶
    ## $ avg_mu_hat_reject <dbl> 0.09830947, 2.27241568, 2.61868664, 3.18812983, 4.01‚Ä¶

``` r
ggplot(summary_results, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power Curve for One-Sample t-test (n = 30, ùúé=5)",
    x = "True mean (Œº)",
    y = "Power (Probability of Rejecting H0)"
  ) +
  theme_minimal()
```

![](p8105_hw5_xc2784_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

There is a positive association between effect size and power, that
larger true effects are easier to detect.

``` r
ggplot(summary_results, aes(x = mu)) +
  geom_line(aes(y = avg_mu_hat, color = "All samples")) +
  geom_line(aes(y = avg_mu_hat_reject, color = "Only rejected H0")) +
  labs(
    title = "Average Estimated Mean vs. True Mean",
    x = "True mean (Œº)",
    y = expression(Average~hat(mu))
  ) +
  theme_minimal()
```

![](p8105_hw5_xc2784_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

No, the sample average of ùúáÃÇ across tests for which the null is rejected
is not approximately equal to the true value of ùúá, especially for
smaller effect sizes. When we only look at the cases that achieved
statistical significance, we‚Äôre essentially selecting the samples where
random variation happened to make the estimated mean unusually large.
This selective process causes the average estimate to be biased upward
compared to the true mean. As the true mean increases and the test gains
more power, nearly all samples become significant, and this bias become
smaller.

### problem 3

``` r
raw <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.
    ## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
glimpse(raw)
```

    ## Rows: 52,179
    ## Columns: 12
    ## $ uid           <chr> "Alb-000001", "Alb-000002", "Alb-000003", "Alb-000004", ‚Ä¶
    ## $ reported_date <dbl> 20100504, 20100216, 20100601, 20100101, 20100102, 201001‚Ä¶
    ## $ victim_last   <chr> "GARCIA", "MONTOYA", "SATTERFIELD", "MENDIOLA", "MULA", ‚Ä¶
    ## $ victim_first  <chr> "JUAN", "CAMERON", "VIVIANA", "CARLOS", "VIVIAN", "GERAL‚Ä¶
    ## $ victim_race   <chr> "Hispanic", "Hispanic", "White", "Hispanic", "White", "W‚Ä¶
    ## $ victim_age    <chr> "78", "17", "15", "32", "72", "91", "52", "52", "56", "4‚Ä¶
    ## $ victim_sex    <chr> "Male", "Male", "Female", "Male", "Female", "Female", "M‚Ä¶
    ## $ city          <chr> "Albuquerque", "Albuquerque", "Albuquerque", "Albuquerqu‚Ä¶
    ## $ state         <chr> "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "N‚Ä¶
    ## $ lat           <dbl> 35.09579, 35.05681, 35.08609, 35.07849, 35.13036, 35.151‚Ä¶
    ## $ lon           <dbl> -106.5386, -106.7153, -106.6956, -106.5561, -106.5810, -‚Ä¶
    ## $ disposition   <chr> "Closed without arrest", "Closed by arrest", "Closed wit‚Ä¶

Description: Each row in the dataset represents one homicide that
occurred between 2007 and 2017 in one of 50 major U.S. cities. There are
52179 observations and 12 variables in total.

``` r
homicides <- raw %>%
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved)
  )

homicides
```

    ## # A tibble: 51 √ó 3
    ##    city_state      total_homicides unsolved_homicides
    ##    <chr>                     <int>              <int>
    ##  1 Albuquerque, NM             378                146
    ##  2 Atlanta, GA                 973                373
    ##  3 Baltimore, MD              2827               1825
    ##  4 Baton Rouge, LA             424                196
    ##  5 Birmingham, AL              800                347
    ##  6 Boston, MA                  614                310
    ##  7 Buffalo, NY                 521                319
    ##  8 Charlotte, NC               687                206
    ##  9 Chicago, IL                5535               4073
    ## 10 Cincinnati, OH              694                309
    ## # ‚Ñπ 41 more rows

``` r
baltimore <- homicides %>%
  filter(city_state == "Baltimore, MD")

baltimore_test <- prop.test(
  x = baltimore$unsolved_homicides,
  n = baltimore$total_homicides
)

baltimore_tidy <- tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high)

baltimore_tidy
```

    ## # A tibble: 1 √ó 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663
